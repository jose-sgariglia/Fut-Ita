{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from tqdm import tqdm as progressbar\n",
    "from utils.constant import *\n",
    "\n",
    "print(\"Loading Datasets...\")\n",
    "datasets = []\n",
    "\n",
    "for filename in progressbar(os.listdir(DIR_RAW_DATA)):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(DIR_RAW_DATA, filename)\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        datasets.append(df)\n",
    "\n",
    "print(\"Datasets loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_datasets(datasets: list[pd.DataFrame], columns_to_keep: list[str] = [\"Territorio\", \"SEXISTAT1\", \"ETA1\", \"TIME\", \"Value\"]) -> list[pd.DataFrame]:\n",
    "    \"\"\"Pulisce una lista di DataFrame secondo criteri specificati.\"\"\"\n",
    "\n",
    "    for i, dataset in progressbar(enumerate(datasets), desc=\"Pulizia dei Dataset\"):\n",
    "        # Mantieni solo le colonne specificate\n",
    "        dataset = dataset[columns_to_keep]\n",
    "\n",
    "        # Rimuovi record indesiderati e stringhe specifiche, converte in numerico\n",
    "        dataset = dataset[(dataset['SEXISTAT1'] != 9) & \n",
    "                          (dataset['ETA1'] != 'TOTAL') & \n",
    "                          (dataset['Territorio'].isin(ITA_STATE.keys()))]\n",
    "        dataset['ETA1'] = pd.to_numeric(dataset['ETA1'].replace({'Y_GE100': '100'}).str.replace('Y', ''), errors='coerce')\n",
    "        dataset.dropna(subset=['ETA1'], inplace=True)\n",
    "\n",
    "        # Verifica tipo della colonna\n",
    "        if not pd.api.types.is_integer_dtype(dataset['ETA1']):\n",
    "            raise ValueError(\"La colonna ETA1 contiene ancora tipi non int\")\n",
    "        \n",
    "        datasets[i] = dataset\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "\n",
    "print(\"Cleaning Datasets...\")\n",
    "\n",
    "cleaned_datasets = cleaning_datasets(datasets)\n",
    "\n",
    "print(\"Datasets cleaned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_datasets(datasets: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"Unisce una lista di DataFrame in un unico DataFrame.\"\"\"\n",
    "\n",
    "    def rename_columns(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Rinomina le colonne del DataFrame con nomi più riconoscibili.\"\"\"\n",
    "\n",
    "        new_column_names = {\n",
    "            \"Territorio\": \"Territory\",\n",
    "            \"SEXISTAT1\": \"Sex\",\n",
    "            \"TIME\": \"Year\"\n",
    "        }\n",
    "\n",
    "        return dataset.rename(columns=new_column_names)\n",
    "    \n",
    "    def convert_unsupported_types(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Converte i tipi di dati non supportati da Parquet in tipi supportati.\"\"\"\n",
    "        for col in dataset.columns:\n",
    "            dtype = dataset[col].dtype\n",
    "            if isinstance(dtype, pd.CategoricalDtype) or pd.api.types.is_object_dtype(dataset[col]):\n",
    "                dataset[col] = dataset[col].astype(str)\n",
    "            elif isinstance(dtype, pd.IntervalDtype):\n",
    "                dataset[col] = dataset[col].apply(lambda x: str(x) if pd.notnull(x) else None)\n",
    "        return dataset\n",
    "\n",
    "    def sort_by_year(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Ordina il DataFrame in base alla colonna TIME.\"\"\"\n",
    "        if not pd.api.types.is_numeric_dtype(dataset['TIME']):\n",
    "            dataset['TIME'] = pd.to_numeric(dataset['TIME'], errors='coerce')\n",
    "        return dataset.sort_values(by='TIME', ascending=True)\n",
    "\n",
    "    merged_dataset = pd.concat(datasets, ignore_index=True)\n",
    "    merged_dataset = convert_unsupported_types(merged_dataset)\n",
    "    merged_dataset = sort_by_year(merged_dataset)\n",
    "    merged_dataset = rename_columns(merged_dataset)\n",
    "    merged_dataset = merged_dataset.drop_duplicates()\n",
    "\n",
    "    return merged_dataset\n",
    "\n",
    "print(\"Merging Datasets...\")\n",
    "merged_dataset = merge_datasets(cleaned_datasets)\n",
    "print(\"Datasets merged.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_range(dataset: pd.DataFrame, age_bins: list[int] = [0, 9, 19, 29, 39, 49, 59, 69, 79, 89, 99, 109]) -> pd.DataFrame:\n",
    "    \"\"\"Crea gruppi di età e aggiunge colonne di età raggruppate con nomi leggibili.\"\"\"\n",
    "    age_groups = pd.cut(dataset['ETA1'], bins=age_bins, right=False)\n",
    "\n",
    "    pivot_df = dataset.pivot_table(\n",
    "        index=['Year', 'Sex'],\n",
    "        columns=age_groups, \n",
    "        values='Value', \n",
    "        aggfunc='sum', \n",
    "        fill_value=0, \n",
    "        observed=False\n",
    "    ).reset_index()\n",
    "\n",
    "    format_name = \"Age_{}_{}\"\n",
    "    pivot_df.columns = ['Year', 'Sex'] + [format_name.format(interval.left, interval.right) for interval in pivot_df.columns[2:]]\n",
    "\n",
    "    return pd.merge(dataset.drop(columns=['Value', 'ETA1']), pivot_df, on=['Year', 'Sex'], how='left')\n",
    "\n",
    "def age_bins_percentage(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Calcola la percentuale di ogni fascia di età rispetto alla popolazione totale.\"\"\"\n",
    "    age_columns = dataset.filter(like='Age_').columns\n",
    "    dataset['Total_Population'] = dataset[age_columns].sum(axis=1)\n",
    "    for col in age_columns:\n",
    "        dataset['Perc_' + col] = dataset[col] / dataset['Total_Population'] * 100\n",
    "    return dataset\n",
    "\n",
    "def population_growth_rate(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    dataset = dataset.sort_values(by=['Territory', 'Sex', 'Year'])\n",
    "    dataset = dataset.fillna(0)\n",
    "    dataset['Growth_Rate_Total_Population'] = dataset.groupby(['Territory', 'Sex'])['Total_Population'] \\\n",
    "                                                   .pct_change() * 100\n",
    "\n",
    "    for age_group in dataset.columns:\n",
    "        if age_group.startswith('Age_'):\n",
    "            dataset[f'Growth_Rate_{age_group}'] = dataset.groupby(['Territory', 'Sex'])[age_group] \\\n",
    "                                                         .pct_change() * 100\n",
    "\n",
    "\n",
    "    dataset['Growth_Rate_Total_Population'] = dataset.groupby(['Territory', 'Sex'])['Growth_Rate_Total_Population'] \\\n",
    "                                                   .transform(lambda x: x.fillna(0))\n",
    "\n",
    "    for age_group in dataset.columns:\n",
    "        if age_group.startswith('Growth_Rate_Age_'):\n",
    "            dataset[age_group] = dataset.groupby(['Territory', 'Sex'])[age_group] \\\n",
    "                                        .transform(lambda x: x.fillna(0))\n",
    "\n",
    "    dataset = dataset.drop_duplicates(subset=['Year', 'Territory', 'Sex'])\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def cumulative_population(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Calcola la somma cumulativa della popolazione per ciascuna fascia di età.\"\"\"\n",
    "    age_columns = dataset.filter(like='Age_').columns\n",
    "    dataset['Cumulative_Pop'] = dataset[age_columns].cumsum(axis=1).iloc[:, -1]\n",
    "    return dataset\n",
    "\n",
    "# Aggiorna il dizionario delle funzioni di feature\n",
    "features_function = {\n",
    "    \"age_range\": age_range,\n",
    "    \"age_bins_percentage\": age_bins_percentage,\n",
    "    \"population_growth_rate\": population_growth_rate,\n",
    "    \"cumulative_population\": cumulative_population\n",
    "}\n",
    "\n",
    "def apply_feature_engineering(dataset: pd.DataFrame) -> list[pd.DataFrame]:\n",
    "    \"\"\"Applica tutte le funzioni di feature engineering definite a ciascun dataset.\"\"\"\n",
    "    for feature_name, feature_function in progressbar(features_function.items()):\n",
    "        dataset = feature_function(dataset)  # Applica ogni funzione di feature engineering\n",
    "    \n",
    "    dataset = dataset.drop_duplicates()\n",
    "\n",
    "    return dataset\n",
    "\n",
    "print(\"Feature Engineering...\")\n",
    "merged_dataset.to_csv(\"merged_dataset.csv\")\n",
    "#featured_dataset = apply_feature_engineering(merged_dataset)\n",
    "print(\"Feature Engineering done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(dataset: pd.DataFrame, output_dir: str = DIR_CLEANED_DATA, output_filename: str = f\"merged_dataset\", save_to_csv: bool = False):\n",
    "    \"\"\"Salva un DataFrame in formato Parquet e CSV (opzionale).\"\"\"\n",
    "    if not os.path.exists(output_dir):  \n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    file_path = os.path.join(output_dir, output_filename)\n",
    "    dataset.to_parquet(file_path + \".parquet\", index=False)\n",
    "    if save_to_csv:\n",
    "        dataset.to_csv(file_path + \".csv\", index=False)\n",
    "\n",
    "\n",
    "def save_multiple_datasets(dataset: pd.DataFrame, output_dir: str = DIR_CLEANED_DATA, output_filename: str = f\"dataset\", save_to_csv: bool = False):\n",
    "    for territory, code in progressbar(ITA_STATE.items(), desc=\"Saving Multiple Datasets\"):\n",
    "        territory_dataset = dataset[dataset['Territory'] == territory]\n",
    "        save_dataset(territory_dataset, output_dir, f\"{output_filename}_{code}\", save_to_csv)\n",
    "\n",
    "\n",
    "def process_and_save(dataset: pd.DataFrame, output_dir: str = DIR_CLEANED_DATA, output_filename: str = f\"dataset\", save_to_csv: bool = False, multiple_datasets: bool = False):\n",
    "    \"\"\"Processa i dataset e li salva in formato Parquet e CSV (opzionale).\"\"\"\n",
    "    if not os.path.exists(output_dir):  \n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    try:\n",
    "        save_dataset(dataset, output_dir, output_filename, save_to_csv)\n",
    "        if multiple_datasets:\n",
    "            save_multiple_datasets(dataset, output_dir, output_filename, save_to_csv)\n",
    "            print(\"Multiple datasets saved successfully!\")\n",
    "        else:\n",
    "            print(\"Dataset saved successfully!\")\n",
    "\n",
    "        file_path = os.path.join(output_dir, output_filename)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during saving: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"Merging Datasets and Saving...\")\n",
    "\n",
    "process_and_save(featured_dataset, save_to_csv=True, multiple_datasets=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
