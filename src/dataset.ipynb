{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:03<00:00,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from tqdm import tqdm as progressbar\n",
    "from utils.constant import *\n",
    "\n",
    "print(\"Loading Datasets...\")\n",
    "datasets = []\n",
    "\n",
    "for filename in progressbar(os.listdir(DIR_RAW_DATA)):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(DIR_RAW_DATA, filename)\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        datasets.append(df)\n",
    "\n",
    "print(\"Datasets loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/tmp/ipykernel_19276/1222493054.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_prova.sort_values(by=[\"TIME\", \"SEXISTAT1\", \"ETA1\"], ascending=True, inplace=True)\n",
      "2it [00:00, 33.22it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/python_project/Fut-Ita/venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: False",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 37\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m datasets\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNormalizing datasets...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m normalized_datasets \u001b[38;5;241m=\u001b[39m \u001b[43mnormalizing_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasets normalized.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[54], line 22\u001b[0m, in \u001b[0;36mnormalizing_datasets\u001b[0;34m(datasets)\u001b[0m\n\u001b[1;32m     19\u001b[0m df_prova\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprova_2.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m tot \u001b[38;5;241m=\u001b[39m df_prova\n\u001b[0;32m---> 22\u001b[0m tot_cal \u001b[38;5;241m=\u001b[39m df_prova[(df_prova[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSEXISTAT1\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m9\u001b[39m) \u001b[38;5;241m&\u001b[39m (df_prova[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mETA1\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTOTAL\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m&\u001b[39m (\u001b[43mdf_prova\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSTATCIV2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m99\u001b[39;49m\u001b[43m]\u001b[49m)][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtot: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtot\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, tot_cal: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtot_cal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m y \u001b[38;5;241m=\u001b[39m df_prova[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[0;32m~/python_project/Fut-Ita/venv/lib/python3.10/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/python_project/Fut-Ita/venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: False"
     ]
    }
   ],
   "source": [
    "def normalizing_datasets(datasets: list[pd.DataFrame]) -> list[pd.DataFrame]:\n",
    "    x, y = 0, 0\n",
    "    for i, df in progressbar(enumerate(datasets)):\n",
    "        if \"CITTADINANZA\" in list(df.columns):\n",
    "            \n",
    "            df_prova = df[(df[\"TIME\"] == 2018) & (df[\"Territorio\"] == \"Italia\") & (df[\"SEXISTAT1\"] == 9) ]\n",
    "            \n",
    "            df_prova.groupby([\"TIME\", \"SEXISTAT1\", \"ETA1\"])[\"Value\"].sum().reset_index()\n",
    "            \n",
    "            df_prova.sort_values(by=[\"TIME\", \"SEXISTAT1\", \"ETA1\"], ascending=True, inplace=True)\n",
    "            df_prova.to_csv(\"prova_1.csv\")\n",
    "            x = df_prova[\"Value\"].sum()\n",
    "            \n",
    "            df = df[df[\"TIME\"] != 2019]\n",
    "            df = df.drop(columns=[\"Cittadinanza\", \"CITTADINANZA\"])\n",
    "        elif \"STATCIV2\" in list(df.columns):\n",
    "            df_prova = df[(df[\"TIME\"] == 2019) & (df[\"Territorio\"] == \"Italia\")]\n",
    "            df_prova.sort_values(by=[\"TIME\", \"SEXISTAT1\", \"ETA1\"], ascending=True, inplace=True)\n",
    "            df_prova.to_csv(\"prova_2.csv\")\n",
    "            \n",
    "            tot = df_prova[(df_prova[\"SEXISTAT1\"] == 9) & (df_prova[\"ETA1\"] == \"TOTAL\") & (df_prova[\"STATCIV2\"] == 99)]\n",
    "            tot_cal = df_prova[(df_prova[\"SEXISTAT1\"] == 9) & (df_prova[\"ETA1\"] != \"TOTAL\") & (df_prova[\"STATCIV2\" == 99])][\"Value\"].sum()\n",
    "            print(f\"tot: {tot}, tot_cal: {tot_cal}\")\n",
    "            y = df_prova[\"Value\"].sum()\n",
    "            \n",
    "            df = df[df[\"STATCIV2\"] == 99]\n",
    "            df = df.drop(columns=[\"Stato civile\", \"STATCIV2\"])\n",
    "        #datasets[i] = df\n",
    "        \n",
    "    print(f\"x: {x}, y: {y}, x - y: {x - y}\")\n",
    "    return datasets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Normalizing datasets...\")\n",
    "normalized_datasets = normalizing_datasets(datasets)\n",
    "print(\"Datasets normalized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_datasets(datasets: list[pd.DataFrame], columns_to_keep: list[str] = [\"Territorio\", \"SEXISTAT1\", \"ETA1\", \"TIME\", \"Value\"]) -> list[pd.DataFrame]:\n",
    "    \"\"\"Pulisce una lista di DataFrame secondo criteri specificati.\"\"\"\n",
    "\n",
    "    for i, dataset in progressbar(enumerate(datasets), desc=\"Pulizia dei Dataset\"):\n",
    "        # Mantieni solo le colonne specificate\n",
    "        dataset = dataset[columns_to_keep]\n",
    "\n",
    "        # Rimuovi record indesiderati e stringhe specifiche, converte in numerico\n",
    "        dataset = dataset[(dataset['SEXISTAT1'] != 9) & \n",
    "                          (dataset['ETA1'] != 'TOTAL') & \n",
    "                          (dataset['Territorio'].isin(ITA_STATE.keys()))]\n",
    "        dataset['ETA1'] = pd.to_numeric(dataset['ETA1'].replace({'Y_GE100': '100'}).str.replace('Y', ''), errors='coerce')\n",
    "        dataset.dropna(subset=['ETA1'], inplace=True)\n",
    "\n",
    "        # Verifica tipo della colonna\n",
    "        if not pd.api.types.is_integer_dtype(dataset['ETA1']):\n",
    "            raise ValueError(\"La colonna ETA1 contiene ancora tipi non int\")\n",
    "        \n",
    "        datasets[i] = dataset\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "\n",
    "print(\"Cleaning Datasets...\")\n",
    "\n",
    "cleaned_datasets = cleaning_datasets(normalized_datasets)\n",
    "\n",
    "print(\"Datasets cleaned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_datasets(datasets: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"Unisce una lista di DataFrame in un unico DataFrame.\"\"\"\n",
    "\n",
    "    def rename_columns(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Rinomina le colonne del DataFrame con nomi più riconoscibili.\"\"\"\n",
    "\n",
    "        new_column_names = {\n",
    "            \"Territorio\": \"Territory\",\n",
    "            \"SEXISTAT1\": \"Sex\",\n",
    "            \"TIME\": \"Year\"\n",
    "        }\n",
    "\n",
    "        return dataset.rename(columns=new_column_names)\n",
    "    \n",
    "    def convert_unsupported_types(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Converte i tipi di dati non supportati da Parquet in tipi supportati.\"\"\"\n",
    "        for col in dataset.columns:\n",
    "            dtype = dataset[col].dtype\n",
    "            if isinstance(dtype, pd.CategoricalDtype) or pd.api.types.is_object_dtype(dataset[col]):\n",
    "                dataset[col] = dataset[col].astype(str)\n",
    "            elif isinstance(dtype, pd.IntervalDtype):\n",
    "                dataset[col] = dataset[col].apply(lambda x: str(x) if pd.notnull(x) else None)\n",
    "        return dataset\n",
    "\n",
    "    def sort_by_year(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Ordina il DataFrame in base alla colonna TIME.\"\"\"\n",
    "        if not pd.api.types.is_numeric_dtype(dataset['TIME']):\n",
    "            dataset['TIME'] = pd.to_numeric(dataset['TIME'], errors='coerce')\n",
    "        return dataset.sort_values(by='TIME', ascending=True)\n",
    "\n",
    "    merged_dataset = pd.concat(datasets, ignore_index=True)\n",
    "    merged_dataset = convert_unsupported_types(merged_dataset)\n",
    "    merged_dataset = sort_by_year(merged_dataset)\n",
    "    merged_dataset = rename_columns(merged_dataset)\n",
    "    merged_dataset = merged_dataset.drop_duplicates()\n",
    "\n",
    "    return merged_dataset\n",
    "\n",
    "print(\"Merging Datasets...\")\n",
    "merged_dataset = merge_datasets(cleaned_datasets)\n",
    "print(\"Datasets merged.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_range(dataset: pd.DataFrame, age_bins: list[int] = [0, 9, 19, 29, 39, 49, 59, 69, 79, 89, 99, 109]) -> pd.DataFrame:\n",
    "    \"\"\"Crea gruppi di età e aggiunge colonne di età raggruppate con nomi leggibili.\"\"\"\n",
    "    age_groups = pd.cut(dataset['ETA1'], bins=age_bins, right=False)\n",
    "\n",
    "    pivot_df = dataset.pivot_table(\n",
    "        index=['Year', 'Sex'],\n",
    "        columns=age_groups, \n",
    "        values='Value', \n",
    "        aggfunc='sum', \n",
    "        fill_value=0, \n",
    "        observed=False\n",
    "    ).reset_index()\n",
    "\n",
    "    format_name = \"Age_{}_{}\"\n",
    "    pivot_df.columns = ['Year', 'Sex'] + [format_name.format(interval.left, interval.right) for interval in pivot_df.columns[2:]]\n",
    "\n",
    "    return pd.merge(dataset.drop(columns=['Value', 'ETA1']), pivot_df, on=['Year', 'Sex'], how='left')\n",
    "\n",
    "def age_bins_percentage(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Calcola la percentuale di ogni fascia di età rispetto alla popolazione totale.\"\"\"\n",
    "    age_columns = dataset.filter(like='Age_').columns\n",
    "    dataset['Total_Population'] = dataset[age_columns].sum(axis=1)\n",
    "    for col in age_columns:\n",
    "        dataset['Perc_' + col] = dataset[col] / dataset['Total_Population'] * 100\n",
    "    return dataset\n",
    "\n",
    "def population_growth_rate(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    dataset = dataset.sort_values(by=['Territory', 'Sex', 'Year'])\n",
    "    dataset = dataset.fillna(0)\n",
    "    dataset['Growth_Rate_Total_Population'] = dataset.groupby(['Territory', 'Sex'])['Total_Population'] \\\n",
    "                                                   .pct_change() * 100\n",
    "\n",
    "    for age_group in dataset.columns:\n",
    "        if age_group.startswith('Age_'):\n",
    "            dataset[f'Growth_Rate_{age_group}'] = dataset.groupby(['Territory', 'Sex'])[age_group] \\\n",
    "                                                         .pct_change() * 100\n",
    "\n",
    "\n",
    "    dataset['Growth_Rate_Total_Population'] = dataset.groupby(['Territory', 'Sex'])['Growth_Rate_Total_Population'] \\\n",
    "                                                   .transform(lambda x: x.fillna(0))\n",
    "\n",
    "    for age_group in dataset.columns:\n",
    "        if age_group.startswith('Growth_Rate_Age_'):\n",
    "            dataset[age_group] = dataset.groupby(['Territory', 'Sex'])[age_group] \\\n",
    "                                        .transform(lambda x: x.fillna(0))\n",
    "\n",
    "    dataset = dataset.drop_duplicates(subset=['Year', 'Territory', 'Sex'])\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def cumulative_population(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Calcola la somma cumulativa della popolazione per ciascuna fascia di età.\"\"\"\n",
    "    age_columns = dataset.filter(like='Age_').columns\n",
    "    dataset['Cumulative_Pop'] = dataset[age_columns].cumsum(axis=1).iloc[:, -1]\n",
    "    return dataset\n",
    "\n",
    "# Aggiorna il dizionario delle funzioni di feature\n",
    "features_function = {\n",
    "    \"age_range\": age_range,\n",
    "    \"age_bins_percentage\": age_bins_percentage,\n",
    "    \"population_growth_rate\": population_growth_rate,\n",
    "    \"cumulative_population\": cumulative_population\n",
    "}\n",
    "\n",
    "def apply_feature_engineering(dataset: pd.DataFrame) -> list[pd.DataFrame]:\n",
    "    \"\"\"Applica tutte le funzioni di feature engineering definite a ciascun dataset.\"\"\"\n",
    "    for feature_name, feature_function in progressbar(features_function.items()):\n",
    "        dataset = feature_function(dataset)  # Applica ogni funzione di feature engineering\n",
    "    \n",
    "    dataset = dataset.drop_duplicates()\n",
    "\n",
    "    return dataset\n",
    "\n",
    "print(\"Feature Engineering...\")\n",
    "merged_dataset.to_csv(\"merged_dataset.csv\")\n",
    "#featured_dataset = apply_feature_engineering(merged_dataset)\n",
    "print(\"Feature Engineering done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(dataset: pd.DataFrame, output_dir: str = DIR_CLEANED_DATA, output_filename: str = f\"merged_dataset\", save_to_csv: bool = False):\n",
    "    \"\"\"Salva un DataFrame in formato Parquet e CSV (opzionale).\"\"\"\n",
    "    if not os.path.exists(output_dir):  \n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    file_path = os.path.join(output_dir, output_filename)\n",
    "    dataset.to_parquet(file_path + \".parquet\", index=False)\n",
    "    if save_to_csv:\n",
    "        dataset.to_csv(file_path + \".csv\", index=False)\n",
    "\n",
    "\n",
    "def save_multiple_datasets(dataset: pd.DataFrame, output_dir: str = DIR_CLEANED_DATA, output_filename: str = f\"dataset\", save_to_csv: bool = False):\n",
    "    for territory, code in progressbar(ITA_STATE.items(), desc=\"Saving Multiple Datasets\"):\n",
    "        territory_dataset = dataset[dataset['Territory'] == territory]\n",
    "        save_dataset(territory_dataset, output_dir, f\"{output_filename}_{code}\", save_to_csv)\n",
    "\n",
    "\n",
    "def process_and_save(dataset: pd.DataFrame, output_dir: str = DIR_CLEANED_DATA, output_filename: str = f\"dataset\", save_to_csv: bool = False, multiple_datasets: bool = False):\n",
    "    \"\"\"Processa i dataset e li salva in formato Parquet e CSV (opzionale).\"\"\"\n",
    "    if not os.path.exists(output_dir):  \n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    try:\n",
    "        save_dataset(dataset, output_dir, output_filename, save_to_csv)\n",
    "        if multiple_datasets:\n",
    "            save_multiple_datasets(dataset, output_dir, output_filename, save_to_csv)\n",
    "            print(\"Multiple datasets saved successfully!\")\n",
    "        else:\n",
    "            print(\"Dataset saved successfully!\")\n",
    "\n",
    "        file_path = os.path.join(output_dir, output_filename)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during saving: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"Merging Datasets and Saving...\")\n",
    "\n",
    "process_and_save(featured_dataset, save_to_csv=True, multiple_datasets=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
