{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:09<00:00,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from tqdm import tqdm as progressbar\n",
    "from utils.constant import *\n",
    "\n",
    "print(\"Loading Datasets...\")\n",
    "datasets = []\n",
    "\n",
    "for filename in progressbar(os.listdir(DIR_RAW)):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(DIR_RAW, filename)\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        datasets.append(df)\n",
    "\n",
    "print(\"Datasets loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pulizia dei Dataset: 6it [00:01,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets cleaned.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def cleaning_datasets(datasets: list[pd.DataFrame], columns_to_keep: list[str] = [\"Territorio\", \"SEXISTAT1\", \"ETA1\", \"TIME\", \"Value\"]) -> list[pd.DataFrame]:\n",
    "    \"\"\"Pulisce una lista di DataFrame secondo criteri specificati.\"\"\"\n",
    "\n",
    "    for i, dataset in progressbar(enumerate(datasets), desc=\"Pulizia dei Dataset\"):\n",
    "        # Mantieni solo le colonne specificate\n",
    "        dataset = dataset[columns_to_keep]\n",
    "\n",
    "        # Rimuovi record indesiderati e stringhe specifiche, converte in numerico\n",
    "        dataset = dataset[(dataset['SEXISTAT1'] != 9) & \n",
    "                          (dataset['ETA1'] != 'TOTAL') & \n",
    "                          (dataset['Territorio'].isin(ITA_STATE))]\n",
    "        dataset['ETA1'] = pd.to_numeric(dataset['ETA1'].replace({'Y_GE100': '100'}).str.replace('Y', ''), errors='coerce')\n",
    "        dataset.dropna(subset=['ETA1'], inplace=True)\n",
    "\n",
    "        # Verifica tipo della colonna\n",
    "        if not pd.api.types.is_integer_dtype(dataset['ETA1']):\n",
    "            raise ValueError(\"La colonna ETA1 contiene ancora tipi non int\")\n",
    "\n",
    "        datasets[i] = dataset\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "\n",
    "print(\"Cleaning Datasets...\")\n",
    "\n",
    "cleaned_datasets = cleaning_datasets(datasets)\n",
    "\n",
    "print(\"Datasets cleaned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineering...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Engineering: 100%|██████████| 6/6 [00:00<00:00, 10.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineering done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def age_range(dataset: pd.DataFrame, age_bins: list[int] = [0, 9, 19, 29, 39, 49, 59, 69, 79, 89, 99, 109]) -> pd.DataFrame:\n",
    "    \"\"\"Crea gruppi di età e aggiunge le colonne di età raggruppate con nomi leggibili.\"\"\"\n",
    "    \n",
    "    age_groups = pd.cut(dataset['ETA1'], bins=age_bins, right=False)\n",
    "\n",
    "    pivot_df = dataset.pivot_table(\n",
    "        index=['TIME', 'SEXISTAT1'],\n",
    "        columns=age_groups, \n",
    "        values='Value', \n",
    "        aggfunc='sum', \n",
    "        fill_value=0, \n",
    "        observed=False\n",
    "    ).reset_index()\n",
    "\n",
    "    format_name = \"Age_{}_{}\"\n",
    "    pivot_df.columns = ['TIME', 'SEXISTAT1'] + [format_name.format(interval.left, interval.right) for interval in pivot_df.columns[2:]]\n",
    "\n",
    "    return pd.merge(dataset.drop(columns=['Value', 'ETA1']), pivot_df, on=['TIME', 'SEXISTAT1'], how='left')\n",
    "\n",
    "\n",
    "features_function = {\n",
    "    \"age_range\": age_range,\n",
    "}\n",
    "\n",
    "def apply_feature_engineering(datasets: list[pd.DataFrame]) -> list[pd.DataFrame]:\n",
    "    \"\"\"Applica tutte le funzioni di feature engineering definite a ciascun dataset.\"\"\"\n",
    "    for i, dataset in enumerate(progressbar(datasets, desc=\"Feature Engineering\")):\n",
    "        for feature_name, feature_function in features_function.items():\n",
    "            dataset = feature_function(dataset)  # Applica ogni funzione di feature engineering\n",
    "        datasets[i] = dataset.drop_duplicates()  # Rimuovi duplicati dopo tutte le applicazioni\n",
    "\n",
    "    return datasets\n",
    "\n",
    "print(\"Feature Engineering...\")\n",
    "featured_dataset = apply_feature_engineering(cleaned_datasets)\n",
    "print(\"Feature Engineering done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging Datasets and Saving...\n",
      "Dataset salvato come ../data/cleaned/merged_dataset1726155302 in formato CSV e Parquet!\n"
     ]
    }
   ],
   "source": [
    "def merge_datasets(datasets: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"Unisce una lista di DataFrame in un unico DataFrame.\"\"\"\n",
    "\n",
    "    def rename_columns(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Rinomina le colonne del DataFrame con nomi più riconoscibili.\"\"\"\n",
    "\n",
    "        new_column_names = {\n",
    "            \"Territorio\": \"Territory\",\n",
    "            \"SEXISTAT1\": \"Sex\",\n",
    "            \"TIME\": \"Year\"\n",
    "        }\n",
    "\n",
    "        return dataset.rename(columns=new_column_names)\n",
    "    \n",
    "    def convert_unsupported_types(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Converte i tipi di dati non supportati da Parquet in tipi supportati.\"\"\"\n",
    "        for col in dataset.columns:\n",
    "            dtype = dataset[col].dtype\n",
    "            if isinstance(dtype, pd.CategoricalDtype) or pd.api.types.is_object_dtype(dataset[col]):\n",
    "                dataset[col] = dataset[col].astype(str)\n",
    "            elif isinstance(dtype, pd.IntervalDtype):\n",
    "                dataset[col] = dataset[col].apply(lambda x: str(x) if pd.notnull(x) else None)\n",
    "        return dataset\n",
    "\n",
    "    def sort_by_year(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Ordina il DataFrame in base alla colonna TIME.\"\"\"\n",
    "        if not pd.api.types.is_numeric_dtype(dataset['TIME']):\n",
    "            dataset['TIME'] = pd.to_numeric(dataset['TIME'], errors='coerce')\n",
    "        return dataset.sort_values(by='TIME', ascending=True)\n",
    "\n",
    "    merged_dataset = pd.concat(datasets, ignore_index=True)\n",
    "    merged_dataset = convert_unsupported_types(merged_dataset)\n",
    "    merged_dataset = sort_by_year(merged_dataset)\n",
    "    merged_dataset = rename_columns(merged_dataset)\n",
    "\n",
    "    return merged_dataset\n",
    "\n",
    "\n",
    "def process_and_save(datasets: list[pd.DataFrame], output_dir: str = DIR_CLEANED, output_filename: str = f\"merged_dataset{int(time())}\", save_to_csv: bool = False):\n",
    "    \"\"\"Processa i dataset e li salva in formato Parquet e CSV (opzionale).\"\"\"\n",
    "    if not os.path.exists(output_dir):  \n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    merged_dataset = merge_datasets(datasets)\n",
    "\n",
    "    file_path = os.path.join(output_dir, output_filename)\n",
    "    try:\n",
    "        merged_dataset.to_parquet(file_path + \".parquet\", index=False)\n",
    "        if save_to_csv:\n",
    "            merged_dataset.to_csv(file_path + \".csv\", index=False)\n",
    "            print(f\"Dataset salvato come {file_path} in formato CSV e Parquet!\")\n",
    "        else:\n",
    "            print(f\"Dataset salvato come {file_path} in formato Parquet!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante il salvataggio del dataset\")\n",
    "\n",
    "\n",
    "print(\"Merging Datasets and Saving...\")\n",
    "\n",
    "process_and_save(featured_dataset, save_to_csv=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
